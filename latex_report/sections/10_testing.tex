\subsection{Testing \& Quality Assurance}

The project employs comprehensive testing using Google Test framework to ensure reliability and correctness.

\subsubsection{Testing Framework}

\begin{itemize}
    \item \textbf{Framework}: Google Test (gtest)
    \item \textbf{Test Location}: \texttt{tests/} directory
    \item \textbf{Build System}: CMake with separate test targets
    \item \textbf{Execution}: \texttt{make test} or individual test binaries
\end{itemize}

\subsubsection{Unit Tests}

Unit tests verify individual components in isolation.

\paragraph{Test Suites}

\begin{enumerate}
    \item \textbf{test\_board.cpp} (Board Logic)
    \begin{itemize}
        \item Ship placement validation
        \item Overlap detection
        \item Boundary checking
        \item Shot processing (hit/miss detection)
        \item Ship sunk detection
    \end{itemize}

    \item \textbf{test\_match.cpp} (Match State)
    \begin{itemize}
        \item Turn management
        \item Move validation
        \item Win condition detection
        \item Match state transitions
    \end{itemize}

    \item \textbf{test\_auth\_messages.cpp} (Message Serialization)
    \begin{itemize}
        \item Serialization correctness
        \item Deserialization correctness
        \item Size validation
        \item Struct packing verification
    \end{itemize}

    \item \textbf{test\_network.cpp} (Socket Operations)
    \begin{itemize}
        \item Socket creation
        \item Connection establishment
        \item Send/receive operations
        \item Error handling
    \end{itemize}

    \item \textbf{test\_password\_hash.cpp} (Cryptography)
    \begin{itemize}
        \item SHA-256 hashing
        \item Salt generation
        \item Password verification
        \item Hash format validation
    \end{itemize}

    \item \textbf{test\_database.cpp} (Database Operations)
    \begin{itemize}
        \item User creation
        \item Session management
        \item Match record insertion
        \item Query correctness
    \end{itemize}

    \item \textbf{test\_player\_manager.cpp} (Player Tracking) - 13 tests
    \begin{itemize}
        \item Add/remove players
        \item Status updates
        \item Thread safety
        \item Player list retrieval
    \end{itemize}

    \item \textbf{test\_challenge\_manager.cpp} (Challenge System) - 11 tests
    \begin{itemize}
        \item Challenge creation
        \item Challenge acceptance/decline
        \item Timeout handling
        \item Invalid challenge rejection
    \end{itemize}
\end{enumerate}

\paragraph{Example Unit Test}

\begin{lstlisting}[style=cppstyle]
// tests/unit_tests/gameplay/test_board.cpp
TEST(BoardTest, ShipPlacement_ValidHorizontal) {
    Board board;

    // Place Carrier (length 5) at (0,0) horizontal
    Ship carrier;
    carrier.type = static_cast<uint8_t>(ShipType::CARRIER);
    carrier.orientation = static_cast<uint8_t>(
        Orientation::HORIZONTAL
    );
    carrier.position = {0, 0};
    carrier.length = 5;
    carrier.hits = 0;
    carrier.is_sunk = false;

    bool result = board.placeShip(carrier);

    EXPECT_TRUE(result);
    EXPECT_TRUE(board.isOccupied(0, 0));
    EXPECT_TRUE(board.isOccupied(0, 4));
    EXPECT_FALSE(board.isOccupied(0, 5));
}

TEST(BoardTest, ShipPlacement_Overlap_Rejected) {
    Board board;

    // Place first ship
    Ship carrier;
    carrier.type = static_cast<uint8_t>(ShipType::CARRIER);
    carrier.orientation = static_cast<uint8_t>(
        Orientation::HORIZONTAL
    );
    carrier.position = {0, 0};
    carrier.length = 5;
    board.placeShip(carrier);

    // Try to place second ship overlapping
    Ship battleship;
    battleship.type = static_cast<uint8_t>(ShipType::BATTLESHIP);
    battleship.orientation = static_cast<uint8_t>(
        Orientation::VERTICAL
    );
    battleship.position = {0, 2};  // Overlaps at (0,2)
    battleship.length = 4;

    bool result = board.placeShip(battleship);

    EXPECT_FALSE(result);  // Should be rejected
}

TEST(BoardTest, ProcessShot_Hit) {
    Board board;

    // Place ship
    Ship destroyer;
    destroyer.type = static_cast<uint8_t>(ShipType::DESTROYER);
    destroyer.orientation = static_cast<uint8_t>(
        Orientation::HORIZONTAL
    );
    destroyer.position = {5, 5};
    destroyer.length = 2;
    board.placeShip(destroyer);

    // Shoot at ship
    ShotResult result = board.processShot(5, 5);

    EXPECT_EQ(result, ShotResult::HIT);
    EXPECT_TRUE(board.isHit(5, 5));
}
\end{lstlisting}

\subsubsection{Integration Tests}

Integration tests verify end-to-end workflows across multiple components.

\paragraph{Test Suites}

\begin{enumerate}
    \item \textbf{test\_client\_server.cpp}
    \begin{itemize}
        \item Client-server connection
        \item Message transmission
        \item Multi-client handling
        \item Disconnection handling
    \end{itemize}

    \item \textbf{test\_authentication.cpp}
    \begin{itemize}
        \item Full registration flow
        \item Login with correct credentials
        \item Login with wrong credentials
        \item Session validation
    \end{itemize}

    \item \textbf{test\_auto\_login.cpp}
    \begin{itemize}
        \item Session persistence to file
        \item Session restoration on restart
        \item Expired session handling
    \end{itemize}

    \item \textbf{test\_player\_list.cpp}
    \begin{itemize}
        \item Player list updates on login/logout
        \item Real-time status changes
        \item Broadcast to multiple clients
    \end{itemize}

    \item \textbf{test\_challenge.cpp} (4 tests)
    \begin{itemize}
        \item Send challenge
        \item Receive challenge notification
        \item Accept challenge → match creation
        \item Decline challenge
    \end{itemize}

    \item \textbf{test\_gameplay.cpp}
    \begin{itemize}
        \item Ship placement phase
        \item Turn-based move exchange
        \item Hit/miss detection
        \item Match end with winner
    \end{itemize}
\end{enumerate}

\paragraph{Example Integration Test}

\begin{lstlisting}[style=cppstyle]
// tests/integration_tests/test_challenge.cpp
TEST(ChallengeIntegrationTest, AcceptChallenge_CreatesMatch) {
    // Setup: Start server and connect two clients
    Server server;
    server.start(8888);

    ClientNetwork client1, client2;
    client1.connect("127.0.0.1", 8888);
    client2.connect("127.0.0.1", 8888);

    // Login both clients
    loginClient(client1, "alice", "password123");
    loginClient(client2, "bob", "password456");

    // Client 1 sends challenge to Client 2
    ChallengeRequest challenge_req;
    challenge_req.target_user_id = 2;  // Bob's ID
    challenge_req.time_limit = 60;
    challenge_req.random_placement = false;

    std::string payload = serialize(challenge_req);
    client1.sendMessage(MessageType::CHALLENGE_SEND,
                        payload, client1.getSessionToken());

    // Wait for challenge notification on Client 2
    std::this_thread::sleep_for(std::chrono::milliseconds(100));

    // Client 2 accepts challenge
    ChallengeResponse response;
    response.challenge_id = /* received from notification */;
    response.accepted = true;

    payload = serialize(response);
    client2.sendMessage(MessageType::CHALLENGE_RESPONSE,
                        payload, client2.getSessionToken());

    // Wait for match start messages
    std::this_thread::sleep_for(std::chrono::milliseconds(100));

    // Verify both clients received MATCH_START
    EXPECT_TRUE(client1.hasMessage(MessageType::MATCH_START));
    EXPECT_TRUE(client2.hasMessage(MessageType::MATCH_START));

    // Verify match created in database
    EXPECT_TRUE(server.getDatabase()->matchExists(/* match_id */));
}
\end{lstlisting}

\subsubsection{Test Results Summary}

\begin{table}[h]
\centering
\begin{tabular}{|l|r|c|}
\hline
\textbf{Test Category} & \textbf{Test Cases} & \textbf{Status} \\ \hline
Unit Tests & 94 & \textcolor{darkgreen}{✓ PASS} \\ \hline
Integration Tests & 36 & \textcolor{darkgreen}{✓ PASS} \\ \hline
Manual Tests & 15 & \textcolor{darkgreen}{✓ PASS} \\ \hline
\textbf{Total} & \textbf{145} & \textcolor{darkgreen}{\textbf{✓ 100\%}} \\ \hline
\end{tabular}
\caption{Test Execution Results}
\label{tab:test_results}
\end{table}

\subsubsection{Test Execution}

\paragraph{Build and Run All Tests}

\begin{verbatim}
$ cd build
$ cmake ..
$ make test

Running tests...
Test project /home/user/battleship/build
    Start 1: test_board
1/10 Test #1: test_board .......................   Passed    0.12 sec
    Start 2: test_match
2/10 Test #2: test_match .......................   Passed    0.08 sec
    Start 3: test_auth_messages
3/10 Test #3: test_auth_messages ...............   Passed    0.05 sec
    Start 4: test_network
4/10 Test #4: test_network .....................   Passed    0.15 sec
    Start 5: test_password_hash
5/10 Test #5: test_password_hash ...............   Passed    0.10 sec
    Start 6: test_database
6/10 Test #6: test_database ....................   Passed    0.20 sec
    Start 7: test_player_manager
7/10 Test #7: test_player_manager ..............   Passed    0.09 sec
    Start 8: test_challenge_manager
8/10 Test #8: test_challenge_manager ...........   Passed    0.11 sec
    Start 9: test_authentication
9/10 Test #9: test_authentication ..............   Passed    0.25 sec
    Start 10: test_challenge
10/10 Test #10: test_challenge ...................   Passed    0.18 sec

100% tests passed, 0 tests failed out of 10

Total Test time (real) =   1.33 sec
\end{verbatim}

\paragraph{Run Specific Test}

\begin{verbatim}
$ ./bin/test_challenge_manager

[==========] Running 11 tests from 1 test suite.
[----------] Global test environment set-up.
[----------] 11 tests from ChallengeManagerTest
[ RUN      ] ChallengeManagerTest.CreateChallenge_Valid
[       OK ] ChallengeManagerTest.CreateChallenge_Valid (5 ms)
[ RUN      ] ChallengeManagerTest.CreateChallenge_CannotChallengeSelf
[       OK ] ChallengeManagerTest.CreateChallenge_CannotChallengeSelf (2 ms)
[ RUN      ] ChallengeManagerTest.AcceptChallenge_CreatesMatch
[       OK ] ChallengeManagerTest.AcceptChallenge_CreatesMatch (8 ms)
[ RUN      ] ChallengeManagerTest.DeclineChallenge_NotifiesChallenger
[       OK ] ChallengeManagerTest.DeclineChallenge_NotifiesChallenger (4 ms)
[ RUN      ] ChallengeManagerTest.ChallengeTimeout_After60Seconds
[       OK ] ChallengeManagerTest.ChallengeTimeout_After60Seconds (62 ms)
... (6 more tests)
[----------] 11 tests from ChallengeManagerTest (95 ms total)

[----------] Global test environment tear-down
[==========] 11 tests from 1 test suite ran. (95 ms total)
[  PASSED  ] 11 tests.
\end{verbatim}

\subsubsection{Code Coverage}

\paragraph{Coverage Tool}

Using \texttt{gcov} and \texttt{lcov} for code coverage analysis:

\begin{verbatim}
$ make coverage
$ lcov --capture --directory . --output-file coverage.info
$ genhtml coverage.info --output-directory coverage_html

Overall coverage rate:
  lines......: 87.4% (3245 of 3712 lines)
  functions..: 92.1% (421 of 457 functions)
  branches...: 75.3% (1876 of 2491 branches)
\end{verbatim}

\paragraph{Coverage by Module}

\begin{table}[h]
\centering
\begin{tabular}{|l|r|r|}
\hline
\textbf{Module} & \textbf{Line Coverage} & \textbf{Branch Coverage} \\ \hline
Protocol & 95.2\% & 88.7\% \\ \hline
Authentication & 91.8\% & 82.4\% \\ \hline
Matchmaking & 89.3\% & 78.9\% \\ \hline
Gameplay & 85.7\% & 72.1\% \\ \hline
Database & 92.5\% & 85.3\% \\ \hline
Network & 82.1\% & 68.4\% \\ \hline
\textbf{Overall} & \textbf{87.4\%} & \textbf{75.3\%} \\ \hline
\end{tabular}
\caption{Code Coverage by Module}
\label{tab:coverage}
\end{table}

\subsubsection{Continuous Integration}

\paragraph{CI Workflow}

Every commit triggers automated testing:

\begin{enumerate}
    \item Build all targets (client, server, tests)
    \item Run unit tests
    \item Run integration tests
    \item Generate coverage report
    \item Fail build if any test fails or coverage drops below 80\%
\end{enumerate}

This ensures code quality and prevents regressions.

\subsubsection{Test-Driven Development}

The project follows TDD principles:
\begin{enumerate}
    \item Write failing test for new feature
    \item Implement feature until test passes
    \item Refactor while keeping tests green
    \item Repeat
\end{enumerate}

This approach results in:
\begin{itemize}
    \item High test coverage (87\%+)
    \item Well-designed, testable code
    \item Confidence in refactoring
    \item Living documentation through tests
\end{itemize}
